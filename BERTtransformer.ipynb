{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GEgY-gCG9ujl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "aiYTljMRoUvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "afBSZkeRnNKn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Real Time ML Final Project/data/completeTWEETdataset.csv\")\n",
        "\n",
        "# Convert text labels to numerical values\n",
        "df['label'] = df['account.type'].apply(lambda x: 1 if x=='bot' else 0)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "encoded_data = tokenizer.batch_encode_plus(\n",
        "    df.text.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=256, \n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids = encoded_data['input_ids']\n",
        "attention_masks = encoded_data['attention_mask']\n",
        "labels = torch.tensor(df.label.values)"
      ],
      "metadata": {
        "id": "4YEnmDISntHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "EdDCNWZ4n08i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data loaders\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, \n",
        "                              sampler = RandomSampler(train_dataset), \n",
        "                              batch_size = batch_size)\n",
        "\n",
        "validation_dataloader = DataLoader(val_dataset, \n",
        "                                   sampler = SequentialSampler(val_dataset), \n",
        "                                   batch_size = batch_size)"
      ],
      "metadata": {
        "id": "wY-NpItAn30s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=2,\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "\n",
        "# Set the optimizer and the learning rate\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
      ],
      "metadata": {
        "id": "C73xMtFen7JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fine-tune the model\n",
        "epochs = 2\n",
        "\n",
        "train_loss_values = []\n",
        "val_loss_values = []\n",
        "train_acc_values = []\n",
        "val_acc_values = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch_input_ids = batch[0]\n",
        "        batch_input_mask = batch[1]\n",
        "        batch_labels = batch[2]\n",
        "        model.zero_grad()        \n",
        "        outputs = model(batch_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=batch_input_mask, \n",
        "                        labels=batch_labels)\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        pred = outputs[1]\n",
        "        labels = batch_labels\n",
        "        total_correct += np.sum(np.argmax(pred, axis=1) == labels)\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print('Batch {:>5,}  of  {:>5,}. Loss: {:>0.5f}'.format(step, len(train_dataloader), loss.item()))\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    train_acc = total_correct / len(train_dataset)\n",
        "    train_loss_values.append(train_loss)\n",
        "    train_acc_values.append(train_acc)\n",
        "\n",
        "    print(\"Train loss: {0:.4f}\".format(train_loss))\n",
        "    print(\"Train accuracy: {0:.4f}\".format(train_acc))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    total_eval_correct = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch_input_ids = batch[0]\n",
        "        batch_input_mask = batch[1]\n",
        "        batch_labels = batch[2]\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(batch_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=batch_input_mask, \n",
        "                            labels=batch_labels)\n",
        "            loss = outputs[0]\n",
        "            total_eval_loss += loss.item()\n",
        "            pred = outputs[1]\n",
        "            labels = batch_labels\n",
        "            total_eval_correct += np.sum(np.argmax(pred, axis=1) == labels)\n",
        "\n",
        "    val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    val_acc = total_eval_correct / len(val_dataset)\n",
        "    val_loss_values.append(val_loss)\n",
        "    val_acc_values.append(val_acc)\n",
        "\n",
        "    print(\"Val loss: {0:.4f}\".format(val_loss))\n",
        "    print(\"Val accuracy: {0:.4f}\".format(val_acc))\n",
        "\n",
        "# Plot the train and validation loss and accuracy\n",
        "plt.plot(train_loss_values, label='Train Loss')\n",
        "plt.plot(val_loss_values, label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_acc_values, label='Train Acc')\n",
        "plt.plot(val_acc_values, label='Val Acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnpzIIneoBAR",
        "outputId": "76e9f04e-44db-401b-a0a1-8600c413249c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 2 ========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "EFJEaDmGWAFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Real Time ML Final Project\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "gbvf64UNvFHh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input text\n",
        "text = \"just aother terrible day at school, excited to get over this\"\n",
        "\n",
        "# Tokenize the input text\n",
        "tokens = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
        "\n",
        "# Make a prediction on the input text\n",
        "outputs = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
        "probs = outputs[0].softmax(1)\n",
        "predicted_class = probs.argmax(1)\n",
        "\n",
        "# Print the predicted class\n",
        "print(predicted_class.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg53yjAlY9yC",
        "outputId": "fbfb6805-59bf-4f8f-d5df-a4bb72c0a5cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    }
  ]
}